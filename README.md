# Gradient-Descent-Optimization-Techniques

This is a project that aims to compare and contrast the performance, stability and accuracies of various gradient descent algorithms.

The analysis includes the following algorithms
1. Batch Gradient Descent
2. Polyack's Momentum
3. Nestrov's Accelerated Gradient Descent
4. RMSProp
5. ADAM

